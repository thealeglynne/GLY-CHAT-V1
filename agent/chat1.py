import os, random, requests, datetime
from dotenv import load_dotenv
from typing import TypedDict
from langgraph.graph import StateGraph, END
from langchain_groq import ChatGroq
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory

# ========================
# 1. ConfiguraciÃ³n
# ========================
load_dotenv()
api_key = os.getenv("GROQ_API_KEY")
serper_api_key = os.getenv("SERPER_API_KEY")

if not api_key:
    raise ValueError("en el .env no hay una api valida de GROQ")

if not serper_api_key:
    raise ValueError("en el .env no hay una api valida de SERPER")

llm = ChatGroq(
    model="llama-3.3-70b-versatile",
    api_key=api_key,
    temperature=0.4,
)

# ========================
# 2. Prompt
# ========================
Prompt_estructura = """
[CONTEXTO]
Hoy es {fecha}.
Eres GLY-AI, un modelo de inteligencia artificial desarrollado por GLYNNE S.A.S.
Tu rol es ser un guÃ­a experto en todo lo relacionado con inteligencia artificial: responder dudas, explicar conceptos, orientar sobre herramientas y tendencias.
No estÃ¡s recolectando informaciÃ³n del usuario; tu objetivo es conversar de forma natural y fluida, generando una interacciÃ³n amigable y educativa.
Si el usuario menciona procesos de automatizaciÃ³n empresarial, sugiere el uso de nuestro mÃ³dulo de anÃ¡lisis y generaciÃ³n de auditorÃ­as.
Si habla sobre bases de datos o manejo de datos, sugiere nuestro analizador de DB.

Responde de forma natural y conversacional:
- MantÃ©n la conversaciÃ³n guiada hacia temas de IA.
- Introduce preguntas tuyas Ãºnicamente si ayudan a profundizar o aclarar conceptos.
- Si hay resultados de bÃºsqueda disponibles, intÃ©gralos sin sonar forzado.
- Si no hay resultados de bÃºsqueda, responde con tu conocimiento previo, indicando brevemente si puede estar desactualizado.
- Prioriza claridad, utilidad y concisiÃ³n, evitando advertencias repetitivas innecesarias.

[BUSQUEDA WEB]
{busqueda}

[MEMORIA]
{historial}

[ENTRADA DEL USUARIO]
Consulta: {mensaje}

[RESPUESTA COMO {rol}]
"""


prompt = PromptTemplate(
    input_variables=["rol", "mensaje", "historial", "busqueda", "fecha"],
    template=Prompt_estructura.strip(),
)

# ========================
# 3. Estado global
# ========================
class State(TypedDict):
    mensaje: str
    rol: str
    historial: str
    respuesta: str
    busqueda: str
    user_id: str

usuarios = {}

def get_memory(user_id: str):
    if user_id not in usuarios:
        usuarios[user_id] = ConversationBufferMemory(
            memory_key="historial", input_key="mensaje"
        )
    return usuarios[user_id]

# ========================
# 4. Nodo de bÃºsqueda Serper
# ========================
def serper_node(state: State) -> State:
    try:
        q = state.get("mensaje", "")
        headers = {"X-API-KEY": serper_api_key, "Content-Type": "application/json"}
        resp = requests.post(
            "https://google.serper.dev/search",
            headers=headers,
            json={"q": q},
        )
        data = resp.json()
        if "organic" in data and len(data["organic"]) > 0:
            resumen = [
                f"{item.get('title')} - {item.get('link','')}"
                for item in data["organic"][:3]
            ]
            state["busqueda"] = " | ".join(resumen)
        else:
            state["busqueda"] = "No hubo resultados"
    except Exception as e:
        state["busqueda"] = f"Error en bÃºsqueda: {e}"
    
    print("DEBUG bÃºsqueda:", state["busqueda"])  # ðŸ‘ˆ Debug
    return state

# ========================
# 5. Nodo agente (usa bÃºsqueda + historial)
# ========================
def agente_node(state: State) -> State:
    memory = get_memory(state.get("user_id", "default"))
    historial = memory.load_memory_variables({}).get("historial", "")
    fecha = datetime.date.today().strftime("%d/%m/%Y")

    # ðŸ”‘ LÃ³gica: decidir si activar bÃºsqueda
    activar_busqueda = any(
        palabra in state["mensaje"].lower()
        for palabra in ["quiÃ©n", "cuÃ¡ndo", "actual", "Ãºltimo", "presidente", "hoy", "noticia", "Ãºltima hora"]
    )

    if activar_busqueda:
        state = serper_node(state)  # ejecuta bÃºsqueda solo si hace falta

  

    texto_prompt = prompt.format(
        rol=state["rol"],
        mensaje=state["mensaje"],
        historial=historial,
        busqueda=state.get("busqueda", ""),
        fecha=fecha,
    )
    respuesta = llm.invoke(texto_prompt).content

    # Guardar en memoria
    memory.save_context({"mensaje": state["mensaje"]}, {"respuesta": respuesta})

    state["respuesta"] = respuesta
    state["historial"] = historial
    return state

# ========================
# 6. Grafo
# ========================
workflow = StateGraph(State)
workflow.add_node("serper", serper_node)
workflow.add_node("agente", agente_node)

workflow.set_entry_point("agente")   # ðŸ‘ˆ ahora empieza en el agente
workflow.add_edge("agente", END)

app = workflow.compile()

# ========================
# 7. CLI
# ========================
print("LLM iniciado con LangGraph + Serper dinÃ¡mico")
user_id = str(random.randint(10000, 90000))
rol = "auditor"
print(f"tu user id es {user_id}")

